# Original Copyright (c), NVIDIA CORPORATION. Modifications Â© Amazon.com

defaults:
  - llama3_70b_pretrain
  - _self_

callbacks:
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.lightning.pytorch.callbacks.GarbageCollectionCallback
    gc_interval_train: 100
    gc_interval_val: 100
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    tp_comm_overlap: False
  - _target_: hyperpod_checkpointless_training.nemo_plugins.fault_injection.HPFaultInjectionCallback
    test_fault_config:
      fault_type: "ipr"
      fault_prob_after_bwd: 0
      fault_prob_between_lock: 0
      fault_prob_during_fwd: 0
      fault_prob_during_bwd: 0
      fault_prob_random: 1
      fault_ranks: [8]
      steps_before_fault: 3
  - _target_: hyperpod_checkpointless_training.nemo_plugins.callbacks.CheckpointlessCallback # Checkpointless changes.
    enable_inprocess: true
    enable_checkpointless: true
    enable_checksum: false
    clean_tensor_hook : true
  - _target_: hyperpod_checkpointless_training.nemo_plugins.progress_bar_callback.HCTProgressBar
    refresh_rate: 1
  - _target_: hyperpod_checkpointless_training.nemo_plugins.datamodule_epoch_callback.DataModuleEpochCallback


resume:
  _target_: hyperpod_checkpointless_training.nemo_plugins.resume.CheckpointlessAutoResume # Checkpointless changes.
  resume_from_directory: ${log.ckpt.dirpath}
  resume_if_exists: true
  resume_past_end: true
  resume_ignore_no_checkpoint: true

strategy:
  num_distributed_optimizer_instances: 2 # Checkpointless changes.

log:
  _target_: nemo.lightning.nemo_logger.NeMoLogger
  name: 'llama3_70b_checkpointless_pretraining'
  log_dir: ${log_dir}
  ckpt:
    _target_: nemo.lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint
    monitor: "step"  # Monitor using step
    mode: "max"  # keep checkpoints with higher step numbers
    save_last: True
    save_top_k: 2
    every_n_train_steps: 20
    # specify the directory to write.
    dirpath : ${log_dir}
    filename: '{model_name}--{step}-{consumed_samples}'
    save_weights_only: False  # Save full checkpoint including optimizer state
    save_optim_on_train_end: True
  tensorboard:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: 'tb_logs'
    name: 'llama3_70b_checkpointless_pretraining'
  wandb: null
