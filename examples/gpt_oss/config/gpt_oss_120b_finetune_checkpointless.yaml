defaults:
  - gpt_oss_120b_finetune
  - _self_

callbacks:
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.lightning.pytorch.callbacks.GarbageCollectionCallback
    gc_interval_train: 5
    gc_interval_val: 5
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    tp_comm_overlap: False
  - _target_: hyperpod_checkpointless_training.nemo_plugins.fault_injection.HPFaultInjectionCallback
    test_fault_config:
      fault_type: "ipr"
      fault_prob_after_bwd: 0
      fault_prob_between_lock: 0
      fault_prob_during_fwd: 0
      fault_prob_during_bwd: 0
      fault_prob_random: 1
      fault_ranks: [8]
      steps_before_fault: 3
  - _target_: hyperpod_checkpointless_training.nemo_plugins.callbacks.CheckpointlessCallback # Checkpointless changes.
    enable_inprocess: true
    enable_checkpointless: true
    enable_checksum: false
    clean_tensor_hook : true
    clean_lightning_module: true
  - _target_: hyperpod_checkpointless_training.nemo_plugins.progress_bar_callback.HCTProgressBar
    refresh_rate: 1
  - _target_: hyperpod_checkpointless_training.nemo_plugins.datamodule_epoch_callback.DataModuleEpochCallback

resume:
  _target_: hyperpod_checkpointless_training.nemo_plugins.resume.CheckpointlessAutoResume
  restore_config:
    _target_: nemo.lightning.RestoreConfig
    path: "<model_weight_path_here>"
    load_artifacts: false
  resume_from_directory: ${log.ckpt.dirpath}
  resume_if_exists: true
  resume_past_end: true
  resume_ignore_no_checkpoint: true

strategy:
  num_distributed_optimizer_instances: 2 # Checkpointless changes.
