# Original Copyright (c), NVIDIA CORPORATION. Modifications Â© Amazon.com

defaults:
  - _self_

trainer:
    accelerator: gpu
    devices: 8
    num_nodes: 2
    max_steps: 1000
    limit_val_batches: null
    limit_test_batches: null
    val_check_interval: 5000
    num_sanity_val_steps: 0
    log_every_n_steps: 1
    accumulate_grad_batches: 1
    use_distributed_sampler: False
    reload_dataloaders_every_n_epochs: 1


strategy:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  expert_model_parallel_size: 8
  context_parallel_size: 1
  num_distributed_optimizer_instances: 2 # align loss curve
  ckpt_torch_dist_multiproc: 2 # 2 worker faster load
  ckpt_parallel_load: True


ddp:
  grad_reduce_in_fp32: True
  overlap_grad_reduce: True
  overlap_param_gather: True
  check_for_nan_in_grad: True
  average_in_collective: True
  suggested_communication_unit_size: 1000000000

plugins:
  _target_: nemo.lightning.pytorch.plugins.MegatronMixedPrecision
  precision: 'bf16-mixed'


callbacks:
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.lightning.pytorch.callbacks.GarbageCollectionCallback
    gc_interval_train: 5
    gc_interval_val: 5
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    tp_comm_overlap: False
  - _target_: hyperpod_checkpointless_training.nemo_plugins.progress_bar_callback.HCTProgressBar
    refresh_rate: 1
  - _target_: hyperpod_checkpointless_training.nemo_plugins.checkpoint_transform_callback.CheckpointTransform

# fit 1 node
data:
  seq_length: 2048
  micro_batch_size: 1
  global_batch_size: 16

model:
  config:
    num_layers: 36
    num_moe_experts : 128
    add_bias_linear: False
    window_size: null
    deterministic_mode: False

log:
  _target_: nemo.lightning.nemo_logger.NeMoLogger
  name: 'gpt_oss_small_120b'
  log_dir: &log_dir ${oc.env:CHECKPOINT_DIR}
  ckpt:
    _target_: nemo.lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint
    monitor: "step"  # Monitor using step
    mode: "max"  # keep checkpoints with higher step numbers
    save_last: True
    save_top_k: 2
    every_n_train_steps: &checkpoint_frequency 20
    # specify the directory to write.
    dirpath : ${log.log_dir}
    filename: '{model_name}--{step}-{consumed_samples}'
    save_weights_only: False  # Save full checkpoint including optimizer state
    save_optim_on_train_end: True
  tensorboard:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: 'tb_logs'
    name: 'gpt_oss_finetune'
  wandb: null

optim:
  config:
    _target_: megatron.core.optimizer.OptimizerConfig
    optimizer: 'adam'
    lr: 1e-4
    weight_decay: 0.1
    fp16: False
    bf16: True
    adam_beta1: 0.9
    adam_beta2: 0.98
    adam_eps: 1e-05
    use_distributed_optimizer: True
    clip_grad: 1.0
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.CosineAnnealingScheduler
    warmup_steps: 50
    constant_steps: 0
    min_lr: 0


resume:
  _target_: nemo.lightning.AutoResume
  restore_config:
    _target_: nemo.lightning.RestoreConfig
    path: "<model_weight_path_here>"
    load_artifacts: false
  resume_from_directory: ${log.ckpt.dirpath}
  resume_if_exists: true
  resume_past_end: true
  resume_ignore_no_checkpoint: true


dataset:
  dataset_path: "<data_path>"
  num_workers: 1
  partition: "train"
  pin_memory: false
  shuffle: false
  drop_last: true
  keep_in_memory: false

mmap:
  cache_dir: "/dev/shm/pdl_cache"
  prefetch_length: 10
  lookback_length: 10
  checkpoint_frequency: *checkpoint_frequency
